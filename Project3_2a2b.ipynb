{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ytpfln0CfMoh",
        "outputId": "06b0da52-ce2f-4256-b5ad-0116405fc5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# I have used my google drive to load the training and testing data\n",
        "# Upload the dataset file in your google drive and change the path to run the below line\n",
        "Labeled_data= pd.read_csv('/content/drive/MyDrive/ML/labeled.csv',dtype='object')\n",
        "Unlabeled_data= pd.read_csv('/content/drive/MyDrive/ML/Unlabeled.csv',dtype='object')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Labeled_data.iloc[:,-1])):\n",
        "    if Labeled_data.iloc[:,-1][i] == ' Plastic ':\n",
        "       Labeled_data.iloc[:,-1][i] = 0\n",
        "    elif Labeled_data.iloc[:,-1][i] == ' Metal ':\n",
        "        Labeled_data.iloc[:,-1][i] = 1\n",
        "    elif Labeled_data.iloc[:,-1][i] == ' Ceramic ':\n",
        "        Labeled_data.iloc[:,-1][i] = 2"
      ],
      "metadata": {
        "id": "eBwqGOPIkA-W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Unlabeled_data.iloc[:,-1])):\n",
        "    if Unlabeled_data.iloc[:,-1][i] == ' Plastic ':\n",
        "       Unlabeled_data.iloc[:,-1][i] = 0\n",
        "    elif Unlabeled_data.iloc[:,-1][i] == ' Metal ':\n",
        "        Unlabeled_data.iloc[:,-1][i] = 1\n",
        "    elif Unlabeled_data.iloc[:,-1][i] == ' Ceramic ':\n",
        "        Unlabeled_data.iloc[:,-1][i] = 2"
      ],
      "metadata": {
        "id": "EQjpybG8k3pK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data= Unlabeled_data.iloc[:,:].values\n",
        "unlabeled_data= unlabeled_data.astype('float64')\n",
        "\n",
        "labeled_data= Labeled_data.iloc[:,:].values\n",
        "labeled_data= labeled_data.astype('float64')"
      ],
      "metadata": {
        "id": "VLCOD_i-hoQs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bMeHgAT3izbZ",
        "outputId": "c5718f0f-4630-4db7-9f28-a54bf15eb3f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rdCGx2_xh04N",
        "outputId": "f58db505-c97c-467d-fce1-be85b94cbb27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the cartesian distance is been calculated in this function\n",
        "def cartesian_function(testdata, datas):\n",
        "    test1=testdata.astype('float64')\n",
        "    data1=datas.astype('float64')\n",
        "    diff1 = np.power(test1 - data1, 2)\n",
        "    difference = np.sum(diff1,axis=1)\n",
        "    final_diff = np.power(difference, 0.5)\n",
        "    return final_diff"
      ],
      "metadata": {
        "id": "kivPzNqCmWH3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The classification of the correct labels are been done in this function\n",
        "# for k=3 and 5 --> if the knn classifer doesnt come with the majority decision , i have classified as it based on k =1.\n",
        "def classify(k, label):\n",
        "    #print(k)\n",
        "    #print(label)\n",
        "    output = label[:k]\n",
        "    m = np.count_nonzero(output == 0)\n",
        "    #print(m)\n",
        "    p = np.count_nonzero(output == 1)\n",
        "    #print(p)\n",
        "    c = np.count_nonzero(output == 2)\n",
        "    #print(c)\n",
        "    prob=[]\n",
        "    prob.append(m/k)\n",
        "    prob.append(p/k)\n",
        "    prob.append(c/k)\n",
        "    #print(prob)\n",
        "    if m > p and m > c:\n",
        "        return 0, prob\n",
        "    elif p > m and p> c:\n",
        "        return 1, prob\n",
        "    elif c > m and c > p:\n",
        "        return 2, prob\n",
        "    else:\n",
        "      output = label[:1].astype(int)\n",
        "      if(output==1):\n",
        "        return 1, prob\n",
        "      elif(output == 2):\n",
        "        return 2, prob\n",
        "      elif (output == 0):\n",
        "        return 0, prob"
      ],
      "metadata": {
        "id": "5JhHkdbbmelA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function defines the knn classification by finding the distance and sorting it. \n",
        "# this functions calls classify function to classify labels\n",
        "def knn(testsample, k, data,y_train):\n",
        "    labels = y_train\n",
        "    distance = cartesian_function(testsample, data)\n",
        "    #print(distance)\n",
        "    final_distance = np.vstack((distance, labels))\n",
        "    final = final_distance.T[:, 0].argsort()\n",
        "    final_sorted_distance = final_distance.T[final]\n",
        "    #print(final_sorted_distance)\n",
        "    final_sorted_labels = final_sorted_distance.T[1]\n",
        "    #print(final_sorted_labels)\n",
        "    return classify(k,final_sorted_labels)"
      ],
      "metadata": {
        "id": "QGtmytBDmfox"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels =[]\n",
        "probabilities=[]\n",
        "predictions =0\n",
        "k=3\n",
        "labeled_features = labeled_data[:,:-1]\n",
        "labeled_labels = labeled_data[:, -1]\n",
        "#print(labeled_data.shape)\n",
        "for i in range(len(unlabeled_data)):\n",
        "      pred, prob=knn(unlabeled_data[:, :-1][i], k, labeled_features,labeled_labels)\n",
        "      #print(pred)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "      #print(pred)\n",
        "      #print(unlabeled_data[:, -1][i])\n",
        "      if(pred == unlabeled_data[:, -1][i].astype(int)):\n",
        "        predictions +=1\n",
        "print(f\"{predictions}/{unlabeled_data.shape[0]} correct predictions \")\n",
        "print(f'performance : ', (predictions/unlabeled_data.shape[0]) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NER2ByNq1KGn",
        "outputId": "e82def64-c220-495e-fca3-0c90ee0d72bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/90 correct predictions \n",
            "performance :  40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of iterations and the number of data items to add in each iteration\n",
        "num_iterations = 15\n",
        "batch_size = 1\n",
        "\n",
        "# Split the labeled data into features and labels\n",
        "labeled_features = labeled_data[:, :-1]\n",
        "#print(labeled_features)\n",
        "labeled_labels = labeled_data[:, -1]\n",
        "labeled_labels= labeled_labels.astype(int)\n",
        "k=3\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    # Train a K-nearest neighbor classifier on the labeled data\n",
        "    # Use the classifier to make predictions on the unlabeled data\n",
        "    for i in range(len(unlabeled_data)):\n",
        "      #print(unlabeled_data[:, :-1][i])\n",
        "      pred, prob=knn(unlabeled_data[:, :-1][i], k, labeled_features,labeled_labels)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "    #print(predicted_labels)\n",
        "    \n",
        "    # Compute the purity of the prediction for each data point\n",
        "    max_probabilities = np.max(probabilities, axis=1)\n",
        "    purity = max_probabilities / np.sum(probabilities, axis=1)\n",
        "    \n",
        "    # Select the subset of data with the highest level of certainty\n",
        "    indices = np.argsort(purity)[-batch_size:]\n",
        "    selected_data = unlabeled_data[indices]\n",
        "    selected_labels = np.array(predicted_labels)[indices.astype(int)]\n",
        "\n",
        "    # Add the selected subset to the labeled dataset and remove them from the unlabeled dataset\n",
        "    labeled_data = np.vstack([labeled_data, np.hstack([selected_data[:,:-1], selected_labels.reshape(-1, 1)])])\n",
        "    unlabeled_data = np.delete(unlabeled_data, indices, axis=0)\n",
        "    \n",
        "    # Train a new classifier on the expanded labeled dataset\n",
        "    labeled_features = labeled_data[:, :-1]\n",
        "    labeled_labels = labeled_data[:, -1]\n",
        "\n",
        "    predictions =0\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    for i in range(len(labeled_features)):\n",
        "      pred, prob=knn(labeled_features[i], k, labeled_features,labeled_labels)\n",
        "      #print(pred)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "      if(pred == labeled_labels[i].astype(int)):\n",
        "        predictions +=1\n",
        "    print(\"For Iteration : \",j+1)\n",
        "    print(f\"{predictions}/{labeled_features.shape[0]} correct predictions \")\n",
        "    print(f'performance : ', (predictions/labeled_features.shape[0]) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "730a2169-52a4-4987-dfaa-9916eb74d377",
        "id": "2_vyyLyG2qMC"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Iteration :  1\n",
            "22/31 correct predictions \n",
            "performance :  70.96774193548387\n",
            "For Iteration :  2\n",
            "23/32 correct predictions \n",
            "performance :  71.875\n",
            "For Iteration :  3\n",
            "24/33 correct predictions \n",
            "performance :  72.72727272727273\n",
            "For Iteration :  4\n",
            "25/34 correct predictions \n",
            "performance :  73.52941176470588\n",
            "For Iteration :  5\n",
            "26/35 correct predictions \n",
            "performance :  74.28571428571429\n",
            "For Iteration :  6\n",
            "27/36 correct predictions \n",
            "performance :  75.0\n",
            "For Iteration :  7\n",
            "29/37 correct predictions \n",
            "performance :  78.37837837837837\n",
            "For Iteration :  8\n",
            "30/38 correct predictions \n",
            "performance :  78.94736842105263\n",
            "For Iteration :  9\n",
            "31/39 correct predictions \n",
            "performance :  79.48717948717949\n",
            "For Iteration :  10\n",
            "32/40 correct predictions \n",
            "performance :  80.0\n",
            "For Iteration :  11\n",
            "33/41 correct predictions \n",
            "performance :  80.48780487804879\n",
            "For Iteration :  12\n",
            "34/42 correct predictions \n",
            "performance :  80.95238095238095\n",
            "For Iteration :  13\n",
            "34/43 correct predictions \n",
            "performance :  79.06976744186046\n",
            "For Iteration :  14\n",
            "35/44 correct predictions \n",
            "performance :  79.54545454545455\n",
            "For Iteration :  15\n",
            "36/45 correct predictions \n",
            "performance :  80.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data= Unlabeled_data.iloc[:,:].values\n",
        "unlabeled_data= unlabeled_data.astype('float64')\n",
        "labeled_data= Labeled_data.iloc[:,:].values\n",
        "labeled_data= labeled_data.astype('float64')"
      ],
      "metadata": {
        "id": "HPatCM4o4iqO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of iterations and the number of data items to add in each iteration\n",
        "num_iterations = 1\n",
        "batch_size = 90\n",
        "\n",
        "# Split the labeled data into features and labels\n",
        "labeled_features = labeled_data[:, :-1]\n",
        "#print(labeled_features)\n",
        "labeled_labels = labeled_data[:, -1]\n",
        "labeled_labels= labeled_labels.astype(int)\n",
        "k=3\n",
        "\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    # Train a K-nearest neighbor classifier on the labeled data\n",
        "    # Use the classifier to make predictions on the unlabeled data\n",
        "    for i in range(len(unlabeled_data)):\n",
        "      #print(unlabeled_data[:, :-1][i])\n",
        "      pred, prob=knn(unlabeled_data[:, :-1][i], k, labeled_features,labeled_labels)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "    #print(predicted_labels)\n",
        "    \n",
        "    # Compute the purity of the prediction for each data point\n",
        "    max_probabilities = np.max(probabilities, axis=1)\n",
        "    purity = max_probabilities / np.sum(probabilities, axis=1)\n",
        "    \n",
        "    # Select the subset of data with the highest level of certainty\n",
        "    indices = np.argsort(purity)[-batch_size:]\n",
        "    selected_data = unlabeled_data[indices]\n",
        "    selected_labels = np.array(predicted_labels)[indices.astype(int)]\n",
        "\n",
        "    # Add the selected subset to the labeled dataset and remove them from the unlabeled dataset\n",
        "    labeled_data = np.vstack([labeled_data, np.hstack([selected_data[:,:-1], selected_labels.reshape(-1, 1)])])\n",
        "    unlabeled_data = np.delete(unlabeled_data, indices, axis=0)\n",
        "    \n",
        "    # Train a new classifier on the expanded labeled dataset\n",
        "    labeled_features = labeled_data[:, :-1]\n",
        "    labeled_labels = labeled_data[:, -1]\n",
        "\n",
        "    predictions =0\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    for i in range(len(labeled_features)):\n",
        "      pred, prob=knn(labeled_features[i], k, labeled_features,labeled_labels)\n",
        "      #print(pred)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "      if(pred == labeled_labels[i].astype(int)):\n",
        "        predictions +=1\n",
        "    print(\"For Iteration : \",j+1)\n",
        "    print(f\"{predictions}/{labeled_features.shape[0]} correct predictions \")\n",
        "    print(f'performance : ', (predictions/labeled_features.shape[0]) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "00f2a5dc-74d2-4599-ed27-de51705a1dfe",
        "id": "arMu5KDL4mEm"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Iteration :  1\n",
            "109/120 correct predictions \n",
            "performance :  90.83333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data= Unlabeled_data.iloc[:,:].values\n",
        "unlabeled_data= unlabeled_data.astype('float64')\n",
        "labeled_data= Labeled_data.iloc[:,:].values\n",
        "labeled_data= labeled_data.astype('float64')"
      ],
      "metadata": {
        "id": "i0ktz2yt3Dnb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of iterations and the number of data items to add in each iteration\n",
        "num_iterations = 18\n",
        "batch_size = 5\n",
        "\n",
        "# Split the labeled data into features and labels\n",
        "labeled_features = labeled_data[:, :-1]\n",
        "#print(labeled_features)\n",
        "labeled_labels = labeled_data[:, -1]\n",
        "labeled_labels= labeled_labels.astype(int)\n",
        "k=3\n",
        "\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    # Train a K-nearest neighbor classifier on the labeled data\n",
        "    # Use the classifier to make predictions on the unlabeled data\n",
        "    for i in range(len(unlabeled_data)):\n",
        "      #print(unlabeled_data[:, :-1][i])\n",
        "      pred, prob=knn(unlabeled_data[:, :-1][i], k, labeled_features,labeled_labels)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "    #print(predicted_labels)\n",
        "    \n",
        "    # Compute the purity of the prediction for each data point\n",
        "    max_probabilities = np.max(probabilities, axis=1)\n",
        "    purity = max_probabilities / np.sum(probabilities, axis=1)\n",
        "    \n",
        "    # Select the subset of data with the highest level of certainty\n",
        "    indices = np.argsort(purity)[-batch_size:]\n",
        "    selected_data = unlabeled_data[indices]\n",
        "    selected_labels = np.array(predicted_labels)[indices.astype(int)]\n",
        "\n",
        "    # Add the selected subset to the labeled dataset and remove them from the unlabeled dataset\n",
        "    labeled_data = np.vstack([labeled_data, np.hstack([selected_data[:,:-1], selected_labels.reshape(-1, 1)])])\n",
        "    unlabeled_data = np.delete(unlabeled_data, indices, axis=0)\n",
        "    \n",
        "    # Train a new classifier on the expanded labeled dataset\n",
        "    labeled_features = labeled_data[:, :-1]\n",
        "    labeled_labels = labeled_data[:, -1]\n",
        "\n",
        "    predictions =0\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    for i in range(len(labeled_features)):\n",
        "      pred, prob=knn(labeled_features[i], k, labeled_features,labeled_labels)\n",
        "      #print(pred)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "      if(pred == labeled_labels[i].astype(int)):\n",
        "        predictions +=1\n",
        "    print(\"For Iteration : \",j+1)\n",
        "    print(f\"{predictions}/{labeled_features.shape[0]} correct predictions \")\n",
        "    print(f'performance : ', (predictions/labeled_features.shape[0]) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8DFJMIYzh2B-",
        "outputId": "965f6923-4c7a-45c8-884c-083df5c885e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Iteration :  1\n",
            "26/35 correct predictions \n",
            "performance :  74.28571428571429\n",
            "For Iteration :  2\n",
            "31/40 correct predictions \n",
            "performance :  77.5\n",
            "For Iteration :  3\n",
            "36/45 correct predictions \n",
            "performance :  80.0\n",
            "For Iteration :  4\n",
            "41/50 correct predictions \n",
            "performance :  82.0\n",
            "For Iteration :  5\n",
            "45/55 correct predictions \n",
            "performance :  81.81818181818183\n",
            "For Iteration :  6\n",
            "50/60 correct predictions \n",
            "performance :  83.33333333333334\n",
            "For Iteration :  7\n",
            "54/65 correct predictions \n",
            "performance :  83.07692307692308\n",
            "For Iteration :  8\n",
            "61/70 correct predictions \n",
            "performance :  87.14285714285714\n",
            "For Iteration :  9\n",
            "66/75 correct predictions \n",
            "performance :  88.0\n",
            "For Iteration :  10\n",
            "71/80 correct predictions \n",
            "performance :  88.75\n",
            "For Iteration :  11\n",
            "76/85 correct predictions \n",
            "performance :  89.41176470588236\n",
            "For Iteration :  12\n",
            "79/90 correct predictions \n",
            "performance :  87.77777777777777\n",
            "For Iteration :  13\n",
            "84/95 correct predictions \n",
            "performance :  88.42105263157895\n",
            "For Iteration :  14\n",
            "88/100 correct predictions \n",
            "performance :  88.0\n",
            "For Iteration :  15\n",
            "93/105 correct predictions \n",
            "performance :  88.57142857142857\n",
            "For Iteration :  16\n",
            "98/110 correct predictions \n",
            "performance :  89.0909090909091\n",
            "For Iteration :  17\n",
            "103/115 correct predictions \n",
            "performance :  89.56521739130436\n",
            "For Iteration :  18\n",
            "108/120 correct predictions \n",
            "performance :  90.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data= Unlabeled_data.iloc[:,:].values\n",
        "unlabeled_data= unlabeled_data.astype('float64')\n",
        "labeled_data= Labeled_data.iloc[:,:].values\n",
        "labeled_data= labeled_data.astype('float64')"
      ],
      "metadata": {
        "id": "7mZDds1G5CHV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of iterations and the number of data items to add in each iteration\n",
        "num_iterations = 5\n",
        "batch_size = 18\n",
        "\n",
        "# Split the labeled data into features and labels\n",
        "labeled_features = labeled_data[:, :-1]\n",
        "#print(labeled_features)\n",
        "labeled_labels = labeled_data[:, -1]\n",
        "labeled_labels= labeled_labels.astype(int)\n",
        "k=3\n",
        "\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    # Train a K-nearest neighbor classifier on the labeled data\n",
        "    # Use the classifier to make predictions on the unlabeled data\n",
        "    for i in range(len(unlabeled_data)):\n",
        "      #print(unlabeled_data[:, :-1][i])\n",
        "      pred, prob=knn(unlabeled_data[:, :-1][i], k, labeled_features,labeled_labels)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "    #print(predicted_labels)\n",
        "    \n",
        "    # Compute the purity of the prediction for each data point\n",
        "    max_probabilities = np.max(probabilities, axis=1)\n",
        "    purity = max_probabilities / np.sum(probabilities, axis=1)\n",
        "    \n",
        "    # Select the subset of data with the highest level of certainty\n",
        "    indices = np.argsort(purity)[-batch_size:]\n",
        "    selected_data = unlabeled_data[indices]\n",
        "    selected_labels = np.array(predicted_labels)[indices.astype(int)]\n",
        "\n",
        "    # Add the selected subset to the labeled dataset and remove them from the unlabeled dataset\n",
        "    labeled_data = np.vstack([labeled_data, np.hstack([selected_data[:,:-1], selected_labels.reshape(-1, 1)])])\n",
        "    unlabeled_data = np.delete(unlabeled_data, indices, axis=0)\n",
        "    \n",
        "    # Train a new classifier on the expanded labeled dataset\n",
        "    labeled_features = labeled_data[:, :-1]\n",
        "    labeled_labels = labeled_data[:, -1]\n",
        "\n",
        "    predictions =0\n",
        "    predicted_labels =[]\n",
        "    probabilities=[]\n",
        "    for i in range(len(labeled_features)):\n",
        "      pred, prob=knn(labeled_features[i], k, labeled_features,labeled_labels)\n",
        "      #print(pred)\n",
        "      predicted_labels.append(pred)\n",
        "      probabilities.append(prob)\n",
        "      if(pred == labeled_labels[i].astype(int)):\n",
        "        predictions +=1\n",
        "    print(\"For Iteration : \",j+1)\n",
        "    print(f\"{predictions}/{labeled_features.shape[0]} correct predictions \")\n",
        "    print(f'performance : ', (predictions/labeled_features.shape[0]) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "69fce7a0-d94f-4255-89ee-4a32ee658663",
        "id": "63NUMoLP5Fiv"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Iteration :  1\n",
            "41/48 correct predictions \n",
            "performance :  85.41666666666666\n",
            "For Iteration :  2\n",
            "58/66 correct predictions \n",
            "performance :  87.87878787878788\n",
            "For Iteration :  3\n",
            "74/84 correct predictions \n",
            "performance :  88.09523809523809\n",
            "For Iteration :  4\n",
            "92/102 correct predictions \n",
            "performance :  90.19607843137256\n",
            "For Iteration :  5\n",
            "105/120 correct predictions \n",
            "performance :  87.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of the data is high when we pass the test data to the model and predict and add that test data as training data. doing so we are gradually incresing the traindata.\n",
        "\n",
        "Self training method is vulnerabe to outliers. since we take a 'k' neighbors much small with in the dataset, and first 20 training data couls be biased and model starts to predict according to initial selection training data.\n",
        "\n",
        "The result is bad when we pass all the test data at once and slightly better when we pass then batch wise and add the predicted to train.\n",
        "\n",
        "since self training is so vulnerable co-training is another method to overcome this issue."
      ],
      "metadata": {
        "id": "n7XClzjAyh3K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xH5XGIGGykfB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}